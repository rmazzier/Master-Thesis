%!TEX root = ../dissertation.tex
\chapter{Conclusion}
\label{conclusion}
%In this work, we explored and remarked the importance of the field of XAI.... Described KENN, and saw its capabilities on the collective classification task. We also propose ways of deriving explanations from KENN, by extracting the delta vectors to assess the contribution of single clauses... 
In this work we first gave a review of XAI methods, and distinguished two main approaches for explainability. The first is transparency, which refers to the particular property of models to be interpreted by human supervisors without the need of external tools or complex post-elaboration methods. Transparency is desirable since allows us to have a precise idea of how predictions are obtained, by knowing the internal mechanisms of the models. One drawback, however, is that, typically, for a model to be transparent it is also required to be relatively simple and to have few parameters; this goes against the latest finding in DL research, where deeper models seem to be the more performing ones. On the other hand, post-hoc explainability focuses on extracting explanations from any kind of trained model, which is free to retain its black box nature. These approaches are more complex and often require to solve additional optimization problems, but their ability to be applied to any trained model makes the performance sacrifice no longer necessary.

We then described KENN, a special residual layer designed to inject prior knowledge, expressed as a set FOL clauses, inside a base NN classifier. Specifically, its objective is to improve the base NN predictions by maximizing the truth values of all the rules in the knowledge base. This is made possible thanks to TBFs, which, applied to preactivations from the NN, are able increase the truth value of the logical formulas. We also compared KENN with SBR and RNM, two other notable examples of neuro-symbolic architectures, and analyzed their differences, advantages and shortcomings.
A remarkable feature of KENN is its capability to learn the clause weights, parameters associated to each logical rule, which define their importance. We explored this specific feature of KENN by visualizing the relationship between the clause compliances in the training data with the corresponding clause weights, and noticed a linear correlation which gets stronger as the amount of training data increases. This is an evidence of how KENN is able to boost the importance of useful rules, while partially ignoring useless or damaging ones. We also provided experimental results on the task of collective classification on the Citeseer Dataset: specifically, for the inductive learning paradigm, we found that KENN outperforms both SBR and RNM. Finally, we also propose methods to extract explanations from KENN. 
\textcolor{unipd}{First, we saw how it is possible to extract simple and human readable explanations from the CE, a feature that highlights the inherent transparency of the KENN layer}. Then, we saw how to inspect the precise changes caused by any subset of rules from the knowledge base, by simply extracting the delta vectors from their respective CEs. We also propose some examples of custom metrics, which allow us to quantify the positive (or negative) contribution of KENN in the context of local explanations, and also to assess the amount of disagreement between rules in the knowledge base.

Concluding, KENN is a promising technique to integrate prior knowledge inside NNs: differently from other NeSy methods, its is simple and computationally efficient, despite being less general and precise for other aspects (think for example to the way KENN aggregates the deltas, or to the possibility to use only the universal quantifier). The ability of KENN to learn the clause weights can be an interesting subject of further research. For example, KENN could be used as a knowledge extraction method from data: starting from randomly generated rules, KENN can be able to selectively boost the weights of the more useful ones. Also, more experiments on simple unary base knowledge can be made. 
Finally, explainability in KENN is also a promising research topic: for example, in my experience of stage, we started working on an interactive, web-based interface to debug and visualize the results of a KENN model. This kind of tools can be very useful: they can encourage a new approach towards a more explainable AI, and boost popularity of neuro-symbolic models like KENN. 