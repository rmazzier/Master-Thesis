%!TEX root = ../dissertation.tex
\chapter{Introduction}
\label{introduction}

Deep Learning (DL) research has been moving faster and faster in recent years. Deep models continue to deliver state-of-the-art results in a wide variety of AI applications, large companies are investing huge amounts of money on DL research, the general reputation of deep learning continues to grow, and the enthusiasm towards data-driven AI only seems likely to increase with time. However, the limitations of this approach have already been identified and criticized \cite{marcus2018appraisal}: in fact, while the remarkable results achieved by deep neural models are undeniable, their limitations are often underestimated or ignored.

The first critical aspect of DL is its exclusive dependence on data: this learning framework has proven to be very effective for many specific tasks; however, without enough data, DL models still lack the ability to learn even basic concepts, due to their inability to reason in symbolic and abstract terms. For example, if you verbally describe a unicorn to a 5-year-old child, he will likely be able to recognize it on a television program, to explain others what a unicorn is, and perhaps even to produce a representation of what he has learned in a drawing. In contrast, to teach an NN what a unicorn is, one would have to provide it with hundreds or thousands of pictures, each with a label that tells it whether that is a unicorn or not. In simple terms, DL models have not yet been fully integrated with symbolical reasoning and prior knowledge. 
%Even if the training process goes perfectly, the model will be able to perform very restricted tasks.
%Doubts about this paradigm have also been expressed by leading researchers in the field, such as Geoffrey Hinton in \cite{sabour2017dynamic}. 

Another critical aspects of DL models is their black-box nature: deep NNs have millions or even billions of parameters that can't be directly examined and interpreted by humans.
While model transparency is not always necessary, this opaqueness can be dangerous in some critical domains such as medical and financial ones, and poses the risk of heavily biased models, as pointed out in \cite{o2016weapons}.

This and many other critical aspects of DL motivate this work of thesis, which will be focused around two specific active research fields. The first is Explainable AI (XAI), an area of research that focuses on the task of deriving explanations from ML systems with the aim of achieving more transparent models and boosting trust towards data-driven AI. We will then delve deeper into the field of Neural Symbolic Integration, which tackles the challenge of integrating logical knowledge inside NNs. Specifically, this thesis focuses on Knowledge Enhanced Neural Networks (KENN) \cite{daniele2019kenn}, a special kind of NN layer designed to inject logical knowledge inside a neural model, in order to improve its predictive capabilities. We will report theoretical details, as well as experimental results, together with an analysis of the explainability of KENN.

This work of thesis was conducted during my internship at the Bruno Kessler Foundation in Trento, Italy. During this experience, I worked alongside researcher Alessandro Daniele and professor Luciano Serafini on experiments and further research on KENN, which led to the development of this work.

This thesis is organized as follows. In Chapter \ref{chapt:explainability} we will discuss in depth the concept of explainability in Machine Learning and report the last developments in the field of XAI. In Chapter \ref{chapt:kenn} we will focus on KENN. Specifically, in Section \ref{sec:kenn_theoretical_framework} we'll describe the theoretical framework behind the model. In Section \ref{sec:kenn_architecture} we'll see its architecture, and in Section \ref{sec:kenn_experiments} we will report results of the experiments performed on relational data, on a collective classification task. Finally, in Section \ref{sec:explainability_kenn} will discuss about the explainability of KENN and devise ways to derive explanations from it.

