\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{daniele2019kenn}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Knowledge Enhanced Neural Networks}{17}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapt:kenn}{{3}{17}{Knowledge Enhanced Neural Networks}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Theoretical Framework}{17}{section.3.1}\protected@file@percent }
\newlabel{sec:theoretical_framework}{{3.1}{17}{Theoretical Framework}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Prior Knowledge and language semantic}{17}{subsection.3.1.1}\protected@file@percent }
\newlabel{example:clause}{{3.2}{18}{Prior Knowledge and language semantic}{equation.3.1.2}{}}
\citation{novak1987first}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}$t$-conorm Functions}{19}{subsection.3.1.2}\protected@file@percent }
\newlabel{eq:lukasiewicz}{{3.4}{20}{Lukasiewicz Negation}{equation.3.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}$t$-conorm Boost Functions}{22}{subsection.3.1.3}\protected@file@percent }
\newlabel{thm:min_tbf}{{3.1.2}{23}{}{theorem.3.1.2}{}}
\newlabel{eq:proof1}{{3.8}{23}{$t$-conorm Boost Functions}{equation.3.1.8}{}}
\newlabel{eq:proof2}{{3.9}{23}{$t$-conorm Boost Functions}{equation.3.1.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Applying TBFs to preactivations}{24}{subsection.3.1.4}\protected@file@percent }
\newlabel{eq:delta_g}{{3.10}{24}{}{equation.3.1.10}{}}
\newlabel{eq:soft_approx_delta}{{3.11}{26}{Applying TBFs to preactivations}{equation.3.1.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces This image illustrates the actual deltas produced by KENN, which are the $\delta ^f$, opposed to the actual delta produced on the activations, which is $\delta ^g$. As we said, $\delta ^g$ is not produced directly by the model but it is indirectly \textit  {induced} by the application of $\delta ^f$ on the preactivations. This also illustrates how, thanks to the shape of the sigmoid activation function, the same delta on the preactivation produces a different delta at the activations level: the closer the preactivations to zero, the highest the modification on the final predictions. \relax }}{27}{figure.caption.15}\protected@file@percent }
\newlabel{fig:preacs_deltas_example}{{3.1}{27}{This image illustrates the actual deltas produced by KENN, which are the $\delta ^f$, opposed to the actual delta produced on the activations, which is $\delta ^g$. As we said, $\delta ^g$ is not produced directly by the model but it is indirectly \textit {induced} by the application of $\delta ^f$ on the preactivations. This also illustrates how, thanks to the shape of the sigmoid activation function, the same delta on the preactivation produces a different delta at the activations level: the closer the preactivations to zero, the highest the modification on the final predictions. \relax }{figure.caption.15}{}}
\newlabel{eq:delta_c}{{3.12}{28}{Applying TBFs to preactivations}{equation.3.1.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Increasing the satisfaction of the Knowledge}{28}{subsection.3.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Summary of all the steps needed to produce $\delta ^c$, the vector of deltas derived from a single clause. We refer to this process as \textit  {clause enhancement}.\relax }}{29}{figure.caption.16}\protected@file@percent }
\newlabel{fig:delta_single_clause}{{3.2}{29}{Summary of all the steps needed to produce $\delta ^c$, the vector of deltas derived from a single clause. We refer to this process as \textit {clause enhancement}.\relax }{figure.caption.16}{}}
\newlabel{eq:deltas_sum}{{3.13}{29}{Increasing the satisfaction of the Knowledge}{equation.3.1.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}KENN Architecture}{30}{section.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Detailed depiction of the Clause Enhancer for the clause.... todo\relax }}{31}{figure.caption.17}\protected@file@percent }
\newlabel{fig:CE}{{3.3}{31}{Detailed depiction of the Clause Enhancer for the clause.... todo\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The KE architecture... todo\relax }}{32}{figure.caption.18}\protected@file@percent }
\newlabel{fig:KE}{{3.4}{32}{The KE architecture... todo\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Representation of relational data inside KENN\relax }}{32}{figure.caption.19}\protected@file@percent }
\newlabel{fig:KENNrelationalrepr}{{3.5}{32}{Representation of relational data inside KENN\relax }{figure.caption.19}{}}
\citation{daniele2019kenn,serafini2016logic}
\citation{serafini2016logic}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}KENN for relational data}{33}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Related Work}{33}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Regularization Approaches}{33}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Logic Tensor Networks}{33}{subsection.3.4.1}\protected@file@percent }
\citation{sen2008collective}
\@writefile{toc}{\contentsline {subsubsection}{Semantic Based Regularization}{35}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Model Based Approaches}{35}{subsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Experiments}{35}{section.3.5}\protected@file@percent }
\citation{marra2020relational}
\citation{lu2003link}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Citeseer Dataset}{36}{subsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}The Prior Knowledge}{36}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Results}{37}{subsection.3.5.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Results of kenn experiments for the inductive case....\relax }}{37}{table.caption.20}\protected@file@percent }
\newlabel{tab:resultsinductive}{{3.1}{37}{Results of kenn experiments for the inductive case....\relax }{table.caption.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Results of kenn experiments for the transductive case....\relax }}{37}{table.caption.21}\protected@file@percent }
\newlabel{tab:resultstransductive}{{3.2}{37}{Results of kenn experiments for the transductive case....\relax }{table.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Deltas for the inductive learning task. $95\%$ confidence intervals.\relax }}{38}{figure.caption.22}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Deltas for the transductive learning task. $95\%$ confidence intervals.\relax }}{38}{figure.caption.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Histograms showing the distribution of the accuracies for all the different $500$ runs, for the inductive case. On the left, the accuracies of the base NN vs accuracies of KENN. On the right the distribution of the difference between the NN accuracy vs KENN accuracy.\relax }}{39}{figure.caption.24}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Histograms showing the distribution of the accuracies for all the different $500$ runs, for the transductive case. On the left, the accuracies of the base NN vs accuracies of KENN. On the right the distribution of the difference between the NN accuracy vs KENN accuracy.\relax }}{40}{figure.caption.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Clause Weights and satisfaction of the rules}{41}{subsection.3.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Explainability in KENN}{41}{section.3.6}\protected@file@percent }
\@setckpt{chapters/chapter3}{
\setcounter{page}{42}
\setcounter{equation}{13}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{9}
\setcounter{table}{2}
\setcounter{DefaultLines}{2}
\setcounter{DefaultDepth}{0}
\setcounter{L@lines}{0}
\setcounter{L@depth}{0}
\setcounter{parentequation}{0}
\setcounter{Item}{19}
\setcounter{Hfootnote}{1}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{30}
\setcounter{eu@}{0}
\setcounter{eu@i}{0}
\setcounter{mkern}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{dirtytalk@qdepth}{0}
\setcounter{definition}{0}
\setcounter{theorem}{0}
\setcounter{corollary}{0}
\setcounter{section@level}{0}
}
