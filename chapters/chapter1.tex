%!TEX root = ../dissertation.tex

\chapter{Explainability in Machine Learning}\label{chapt:explainability}

%TODO Altra possibile roba da aggiungere alla intro, da \cite{samek2019xai}.
%In this match AlphaGo played a move, which was
%classiﬁed as “not a human move” by a renowned Go expert, but which was the
%deciding move for AlphaGo to win the game. AlphaGo did not explain the move,
%but the later play unveiled the intention behind its decision. With explainable AI
%it may be possible to also identify such novel patterns and strategies in domains
%like health, drug development or material sciences, moreover, the explanations
%will ideally let us comprehend the reasoning of the system and understand why
%the system has decided e.g. to classify a patient in a speciﬁc manner or asso-
%ciate certain properties with a new drug or material. This opens up innumerable
%possibilities for future research and may lead to new scientiﬁc insights.

In the last years, thanks to the availability of always larger datasets and due to the always growing computing power of modern GPUs, a lot of artificial intelligence (AI) and deep learning (DL) systems have reached remarkable results in terms of performance on a wide variety of tasks, in some cases largely overcoming human capabilities. Examples are the fields of computer vision, speech recognition or machine translation, which almost always fall into the domain of supervised learning. 
DL, being at the moment the most successful ML approach in supervised learning, has been criticized in \cite{marcus2018appraisal} 
%TODO: aggiungere più references qui... ci sono molti paper che fanno questa critica
for its lack of transparency: in fact, DL systems have millions or even billions of parameters, which are not characterized in human interpretable ways, but only in terms of their position in the network topology. This results in opaque models, to the point that such systems are currently treated almost always as black boxes. DL systems also present other critical issues: in \cite{szegedy2013intriguing} the authors made a neural network misclassify an image by applying an hardly perceptible perturbation, found by maximizing the network’s prediction error. Such adversarial examples have also been found to be somewhat universal and not just the result of overfitting. This poses serious doubts about the ability of NN to learn general representations: indeed, if such networks can generalize well, how can they be confused by nearly indistinguishable images? Similarly, authors in \cite{nguyen2015fooled} show how deep neural networks are easily fooled into misclassifying inputs with no resemblance to the true category. Adversarial examples are not confined to the field of computer vision: natural language networks can also be fooled as shown in \cite{jia2017adversarial}. Furthermore, it has been found that in several applications, DL systems present strong biasedness. One example is reported in \cite{bolukbasi2016debiasing}, where the authors show how word embeddings trained on Google News articles exhibit strong female/male gender stereotypes due to biases in the training data. Susceptibility to unintuitive errors remains therefore a pervasive problem in DL and no robust solution has been found for them so far. Such issues contribute to generate mistrust, and threaten to slow down or even hinder the prevailance of AI in some applications, due to the high potential of unexpected behavior and lack of verifiability of solutions.
%TODO: trovare e inserire altri esempi problematici .... questi potrebbero essere vecchi
In the light of such problems, explainable artificial intelligence (XAI) has become an area of interest in the research community: it tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. The need for XAI is now even more urgent: the renewed EU General Data Protection Regulation (GDPR) could require AI providers to provide users with explanations of the results of ML systems based on their personal data. This clearly affects the industry in a huge way: indeed, the GDPR may hinder or even prohibit the use of \say{black box} models which don't offer explanations for their decisions, when based on users' personal data (think for example to recommender systems). This is also referred to as the \say{right to explanation}. The need for XAI has been also expressed by the statement on algorithmic transparency and accountability released by the Association for Computing Machinery \cite{acm2017transparency}, and by the XAI program launched by DARPA in 2017 \cite{gunning2019xai}.

Even though the general aim for XAI is well understood as the achievement of \textit{interpretability}, or \textit{explainability}, for ML models, few articulate precisely what those terms mean or why they are important. Therefore, the first steps of this chapter will be to define What is explainability, Who needs it, When it is needed and Why.

\section{Why do we need explainability}
Before determining a good definition of \textit{explainability} or \textit{interpretability}, we must have a good understanding of what the real world objectives of XAI research are. Consider a supervised learning scenario: a lot of evaluation metrics are used to assess the quality of a model, accuracy probably being  the most common one. The computation of such metrics require the presence of model predictions, together with ground truth labels, in order to produce a score. Unfortunately, this evaluation framework is failing to satisfy the request for interpretability: therefore we should investigate what are those other desiderata which are being demanded, and what are the circumstances in which they are sought. Lipton \cite{lipton2017mythos} suggests that this kind of situations arise \say{when our real world objectives are difficult to encode as simple real-valued functions}: in this sense, \textit{interpretations} are useful to achieve objectives which are important for us, but which we struggle to model in a formal way. Other mentioned motivating aspects are causality, transferability, informativeness and fair and ethical decision making.
%TODO: should i go into more details on those? doesnt seem very full of information...
 The authors in \cite{doshivelez2017rigorous} refer to this same concept as \textit{incompleteness} in the problem formalization: in such situations, explanations are one of ways to ensure human intervention on such questions, where machines are still struggling to understand. Some examples of such scenarios would be:
\begin{itemize}
	\item \textbf{Scientific Understanding}: humans learn about the world around them in the form of knowledge, which is still difficult to formalize in the same way in which it works inside our brains. For this reason, we might look for explanations from ML models, which in turn we can interpret and transform into human interpretable knowledge.
	\item \textbf{Safety}: in complex tasks, rigorous and complete testing is almost never feasible and it is not possible to formally model all the wrong or dangerous decisions that a model can make. For this reason, when decisions from a ML system can pose a threat to others, explanations can help humans to evaluate safety conditions and to boost trust towards AI.
	\item \textbf{Ethics}: for us humans, evaluating the fairness of a decision is often easy, in the same way in which we have a clear idea of how we would want our model to be ethical (for example, we could desire a \say{fair} classifier for loan approval). This kind of properties are not easy to encode in ML systems and at, the same time, biases in the data can often to unethical decisions if not treated properly. 
\end{itemize}

Some papers \cite{Kim2015InteractiveAI, ribeiro2016trust} also motivate the need for explainability and interpretability in light of the need for trust by domain experts: indeed trust is fundamental if one plans to act based on a prediction, therefore ML systems must be able to communicate with highly skilled human experts to leverage their expertise and share useful information or patterns from the data. 
\section{When do we need explainability}
Explainability is important in a lot of domains, but not in all of them. There are applications, e.g. aircraft collision avidance, in which algorithms have been functioning from years without giving any explanations and without any human interaction. It is clear then that ML systems can be used without any need for interpretations in real world applications, in those cases where their raw performance in terms of accuracy suffices, or when the risk of error doesn't pose any serious threat to the end users. Therefore, domains that demand explainability are generally characterized by the critical nature of decisions which need to be made, where mistakes could have severe consequences. Authors in \cite{burkart2021survey} provide a fairly exhaustive list of domains in need for explainability:
%TODO: think better about how to describe each field
\begin{itemize}
	\item Medical Domain/Health-Care: 
	\item Judicial System
	\item Banking/Finance
	\item Bio-informatics
	\item Automobile Industry
	\item Marketing
	\item Election Campaigns
	\item Precision Agriculture
	\item Expert Systems for the Military
	\item Recommender Systems
\end{itemize}
\section{What is explainability}
Several research works attempt to describe rigorously the meaning of explainability in the contest of XAI. In the literature, such word is often used interchangeably or substituted by \say{intrepretability}, even though some try to make a distinction. In \cite{gilpin2018explaining}, for example, the authors claim that explainability is a property of a model that implies interpretability, but not viceversa. More specifically, \textit{interpretability} is described as the capability of a certain model to be described in a  way that is understandable to humans; on the other hand, \textit{explainability} is the property of a model to be able to summarize the reasons for their behavior. Explanations, according to the authors, can be evaluated in two ways: according to their \textit{interpretability} (that is, its understandability by a human being) and their \textit{completeness} (that is, the accuracy of the description). Under this definitions, the challenge of XAI is in creating explanations that are both interpretable and complete, even though such characteristics are often opposed to one another. These two features of explanations resemble two important properties of ML models and suggest a similitude: on one hand, the user would desire a simple model with few parameters and at the same time a model able to capture really well the structure of the training data. While both the properties are desirable, they are almost never achievable at the same time. Indeed, as we train simple models, we will probably underfit the data: in the same way, really easy explanations often fail to capture the complexities behind the internal workings of our algorithms. On the other hand, as we add parameters to our model and make it more complex, it will begin to better fit the data: in the same way a really complete explanation will describe accurately the operations of the systems, but will probably result more difficult to understand to a human being. This comparison suggests that, even for explanations, one should allow for a \textit{tradeoff} between interpretability and completeness. The author suggests also that the explanation methods should be evaluated according to how such explanations behave on the curve from maximum interpretability to maximum completeness. Figure \ref{fig:example_explainability} gives a visualzation of this concept. 

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{figures/example_explainability.png}
	\caption{Example of two different explanations according to the definition given by \cite{gilpin2018explaining}: each explanation allows for a tradeoff between completeness and interpretability. Explanation on the right should be evaluated better than the one on the left: indeed, for higher values of interpretability, the right one offers higher values of completeness.}
	\label{fig:example_explainability}
\end{figure}

In \cite{doshivelez2017rigorous} interpretability is defined as the ability to explain or to present in understandable terms to a human. According to \cite{burkart2021survey}, instead, \textit{interpretability} is most often used in terms of comprehending how the prediction model works as a whole, while \textit{explainability}, in contrast, is used to indicate the capability of models to give explanations about their decision, but keeping their black box nature. In the particular context of generating explanations for DL models, the authors of \cite{montavon2018methods} define an explanation as the collection of features of an interpretable domain (i.e. pixel values on images), that have contributed for a given example to produce a decision (e.g. classification or regression). We can see that none of the aforementioned definitions are specific enough to enable one universal formalization: indeed they implicitly  depend on the context or the aim of the research work. We will now try to give definitions that better summarize the mentioned ones.

\begin{definition}[Interpretability]
	\label{def:interpretability}
	We define \textit{interpretability} in the context of supervised ML as the generic property of a model which makes its single components, as well as its functioning as a whole, understandable by a human being. Examples of interpretable models are simple linear regression or decision trees. Examples of not interpretable models are deep neural networks.
\end{definition}

\begin{definition}[Explainability]
	\label{def:explainability}
	We define \textit{explainability} in the context of supervised ML as the generic property of a model which makes it able to explain, to a certain extent, its own reasons behind a certain decision.
\end{definition}

In general, it is true that an explainable model is almost always interpretable, but the viceversa might not always be true. An exception to this rule, however, even if outside the field of ML, is the human brain: while we are able to give really detailed and motivated reasons behind our decision processes, our brain is not an interpretable model. Indeed, we don't know every single aspect of how our brains works, and yet we (often) trust the explanations that other human beings provide when asked why they took a certain decision. This offers an interesting point of view to the discussion: we should be careful not to trust certain explanations only by the fact that they look plausible and convincing. To this regard, Herman \cite{Herman2017ThePA} warns us, making a clear distinction between descriptive and persuasive explanations: indeed implicit cognitive biases of the human brain could threaten transparency (for example, humans naturally tend to prefer simpler descriptions). To avoid falling in this problem, one should always keep in mind the tradeoff process between completeness and interpretability, mentioned above.

Even after an attempt of definitions, the meaning of interpretability, and explainability is still too generic and can be applied to any ML model. In fact, the volume of research in interpretability is quickly expanding, making the number of available methodologies continuously grow. Despite that, two main categories of approaches to interpretability and explainability are often distinguished \cite{dosilovic2018explainable, lipton2017mythos}: integrated interpretability, or \textit{transparency} and \textit{post-hoc} interpretability.

\subsection{Transparency}

Transparency is one of the properties that can enable interpretability and it implies some sort of understanding of the mechanism by which the model works. It can also be seen as the direct opposite to the concept of \textit{black box}. Lipton \cite{lipton2017mythos} goes into even more details, by subdividing transparency in different levels:
\begin{enumerate}
	\item \textbf{Simulatability}: it's the highest level of abstraction of the concept of transparency. Lipton refers to simulatability as the property of the model that makes it understandable by a person \say{at once}. Specifically, this means that a human could, given the input data and all the necessary parameters, produce a prediction by making all the computations in a reasonable time. 
	\item \textbf{Decomposability}: it's the transparency considered at the level of the single components of the model. Specifically, one model can be considered decomposable if each part of the model (weights, modules, computations...) admits an intuitive explanation.
	\item \textbf{Algorithmic Transparency}: this notion of transparency refers to the learning algorithm itself. For example, we know that in the case of linear regression the shape of the loss function is known, as well as an analytical form for the solution for the problem. This means a maximum degree of algorithmic transparency. On the other hand, modern deep learning lacks this notion of transparency: in fact, even if a lot of powerful optimization algorithms give empirically excellent results, there is no guarantee that those will work on any new problem. The same holds for the shape of the error function, which is almost never known.
\end{enumerate}

It's interesting to notice that the human brain, as noted previously, doesn't exhibit any of those features. In fact, human thinking is not transparent to us and justifications in the form of explanations may differ from the actual decision mechanism.

\subsection{Post-hoc interpretations}

Post hoc interpretability is a different approach to interpretability: it refers to those methods with which we generate explanations from already trained models, without caring about their internal mechanisms. The advantage of this approach is that it does not impact performance of the model, which is treated as a black box. Unlike transparency, this kind of interpretability is the one that applies to humans.
%(trovare il modo di parlare di activation maximization... global vs local..., poi ricordarsi di attaccare il discorso difficoltà di integrare prior knowledge)
Lipton \cite{lipton2017mythos} summarizes post-hoc explainability  methods into four main categories:
\begin{itemize}
	\item \textbf{Text explanations} :This approach aims to mimic the way humans provide explanations to one another, that is, in text form. Example of such approaches: in reinforcement learning \cite{krening2017leaning}, in recommender systems: \cite{mcauley2013hidden}...
	%TODO: search other examples and refine...
	\item \textbf{Visualization}: Another popular approach to post-hoc interpretations is to provide visualizations in order to have a qualitative idea of what the model has learned. For example, when models learn embeddings in high dimensional vector spaces, one popular technique to visualize them is t-SNE \cite{laurens2008tsne}, which provides 2D or 3D visualization of high dimensional data points, in such a way that nearby samples are likely to appear closer together. In the field of computer vision, several papers have investigated the internal representations of visual concepts in CNNs. In \cite{mordvintsev2015inceptionism}, the authors try to build \say{prototipes} of certain objects starting from already trained image classification models. Specifically, starting from a white noise image, they tweak it in such a way that the activation of a certain neuron (in this case, the output neuron corresponding to the selected object) is maximized. This process can be replicated also starting from an existing image, a process which produces really peculiar visual effects (see Figure \ref{fig:image_dream_map}). This process is also known as \textit{Activation Maximization}: consider a deep NN classifier which maps an input tensor (in this case an image) $x$ to a set of classes $\{\omega_i\}_{i=1}^m$. In a classification scenario, we know that the $i$-th output neuron encodes the modeled class probability $p\left(\omega_i|x\right)$. The basic idea is that the \textit{prototype} $x_i^*$, representative of class $\omega_i$ can be found as follows:
	
	$$x_i^* = ´\max_x \log p(\omega_i|x) - \lambda \|x\|^2.$$
	
	The proposed definition doesn't yield good results in practice: although producing strong class response, they often look unnatural. This problem is solved in several ways, for example by adding regularization via a data density model, or imposing prior constraints which are common in real images, such as high correlation among neighboring pixels. Again, a similar approach is found in \cite{mahendran2014understanding}, where the authors investigate the amount of information retained in the hidden representations of CNNs. They manage to reconstruct the original images with good accuracy even from high level representations by performing gradient descent on white noise inputs.
	
	 
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.65\linewidth]{figures/image-dream-map.png}
		\caption{Process of image manipulation shown in \cite{mordvintsev2015inceptionism}. From an intuitive point of view, starting from existing real images, the network is asked to enhance the features of the image that resemble the desired object (in this case, starting from an image of a tree, start looking for features that resemble buildings). This process is then repeated, creating a positive feedback loop. As a result, the features of the desired objects appear seemingly out of nowhere.}
		\label{fig:image_dream_map}
	\end{figure}
	
	\item \textbf{Local explanations}
	%TODO: continue here..
	In \cite{simonyan2013deep, montavon2018methods} saliency maps/relevance scores.
	\cite{ribeiro2016trust} interpretable and faithful manner, by learning an interpretable
	model locally around the prediction...
	Probably also attention mechanisms...
	`
	\item \textbf{Explanation by Example}
\end{itemize}

%\section{Review of XAI methods for deep neural architectures}




