\documentclass{beamer}
\usepackage{dirtytalk}

\usetheme{Padova}
%\setbeamercovered{transparent}
%\usefonttheme{metropolis}

\newcommand{\backupbegin}{
	\newcounter{framenumberappendix}
	\setcounter{framenumberappendix}{\value{framenumber}}
}
\newcommand{\backupend}{
	\addtocounter{framenumberappendix}{-\value{framenumber}}
	\addtocounter{framenumber}{\value{framenumberappendix}} 
}

\title{Towards Explainability in Knowledge Enhanced Neural Networks}
\subtitle{Data Science Master Thesis}
\author{Riccardo Mazzieri \\ \vspace{10pt}
\textbf{Supervisor}: Luciano Serafini\\
\textbf{Co-Supervisor}: Alessandro Daniele \vspace{10pt}}
\date{September 21, 2021}

\definecolor{unipd}{rgb}{.7,.105,.109}
\begin{document}

	\maketitle

	\begin{frame}{Outline}
%		\tableofcontents
\begin{enumerate}
	\item Introduction and motivations \vspace{12pt}
	\item Knowledge Enhanced Neural Networks (KENN) \vspace{12pt}
	\item \textcolor{unipd}{\textbf{Contributions:} Experiments on collective classification} \vspace{12pt}
	\item \textcolor{unipd}{\textbf{Contributions:} Extracting explanations from KENN} \vspace{12pt}
	\item Conclusions
\end{enumerate}
	\end{frame}


	\section{Introduction}

	\begin{frame}{Introduction}
		Deep NNs have several flaws. For example:
		\begin{itemize}
%			\pause
			\item They are \textbf{data hungry}:
			\begin{itemize}
				\item With few data, learning is not possible, even for simple logical reasoning tasks;
				\item This motivates \textbf{Neural Symbolic Integration (NeSy)}.
				\pause
%				research field that focuses on the integration of logic inside neural networks
			\end{itemize}
%			\pause6
			\item They are \textbf{black boxes}:
			\begin{itemize}
				\item Predictions are not explainable, might lead to lack of trust in AI applications;
				\item This motivates the research field of \textbf{Explainable AI (XAI)}.
			\end{itemize}
		\end{itemize}
	\end{frame}


	\section{KENN}

	\begin{frame}{Knowledge Enhanced Neural Networks}
		KENN\footnote{Daniele, Alessandro, and Luciano Serafini. "Knowledge enhanced neural networks." Pacific Rim International Conference on Artificial Intelligence. Springer, Cham, 2019.\vspace{20pt}} consists in a residual layer designed to improve the predictions of a base NN, by using logical prior knowledge, consisting in a set of FOL formulas $\mathcal{K}$.
		\begin{figure}
			\centering
			\includegraphics[width=0.95\linewidth]{images/kenn_intuition.pdf}
			
		\end{figure}
	\end{frame}

\begin{frame}{Basic Terminology}
	\begin{definition}[The Language]
		Our language will be a function-free first order language $\mathcal{L}$, defined by:
		\begin{itemize}
			\item A set of \textbf{constants}: $\mathcal{C}=\{a_1,\dots,a_{|\mathcal{C}|}\}$;
			\item A set of \textbf{predicates}: $\mathcal{P}=\{P_1,\dots,P_{|\mathcal{P}|}\}$;
		\end{itemize}
	\end{definition}

	
	\begin{definition}[Clause]
		A clause $c$ is a formula expressed a disjunction of literals:
		\begin{equation*}
		c:=\bigvee_{i=1}^{k} l_{i}, \quad l_{i} \neq l_{j} \quad \forall i \neq j
		\end{equation*}
	\end{definition}
\end{frame}

%\begin{frame}{Example}
%	Given two predicates $\text{Smoke}$ and $\text{Friends}$, an example of formula is:
%	$$c: \forall x,\forall y \quad \operatorname{Smoke}(x) \wedge \operatorname{Friends}(x,y) \Rightarrow \operatorname{Smoke}(y).$$
%	
%	\pause
%	
%	Inside KENN, formulas are represented as a disjunction of literals, and are evaluated on constants.
%	For example, given two constants $a_1$ and $a_2$, the following clause:
%	
%	$$\neg \operatorname{Smoke}(a_1) \vee \neg \operatorname{Friends}(a_1,a_2) \vee \operatorname{Smoke}(a_2). $$
%	is called the \textbf{grounding} of $c$.
%\end{frame}

\begin{frame}{Language Semantic}
			\begin{figure}
		\centering
		\includegraphics[width=\linewidth]{images/interpretation_intuition0.pdf}
	\end{figure}
	
\end{frame}
\begin{frame}{Language Semantic}
	\begin{figure}
		\centering
		\includegraphics[width=\linewidth]{images/interpretation_intuition1.pdf}
	\end{figure}
\end{frame}

%\begin{frame}{Language Semantic}
%	 Note that truth values can be any real number in $[0,1]$\\ $  \Rightarrow$ We will follow the rules of \textbf{Fuzzy Logic}, an extension of Boolean Logic. Specifically, in Fuzzy logic:
%	\begin{itemize}
%		\pause
%		\item Truth value of a negated predicate is simply $\mathcal{I}(\neg A(x)) = 1 - \mathcal{I}(A(x))$
%		\pause
%		\item Truth value of a disjunction of literals is computed by means of a \textbf{$t$-conorm} function;
%		\pause
%		\item KENN uses the GÃ¶del $t$-conorm, which is defined as:
%		\begin{equation*}
%		\perp_{\text{max}}(t) = \underset{i=1,\dots,m}{\operatorname{max}} t_i, \quad \forall t\in [0,1]^m.
%		\end{equation*}
%	\end{itemize}	
%\end{frame}

%\begin{frame}{Example}
%	\begin{exampleblock}{Example: truth value of a clause}
%		Suppose that, given the constant term $a$, the truth value of $\text{Dog}(a)$ is $0.8$, the one for $\text{Animal}(a)$ is $0.5$ and the one for $\text{Human}(a)$ is $0.3$. Then, the truth value of the grounded clause
%		$$c: \neg \text{Dog}(a) \vee \text{Animal}(a)$$  
%		will be:
%		\begin{equation*}
%		\perp_{\text{max}}(1-0.8,0.5) = \underset{i=1,\dots,m}{\operatorname{max}}(0.2,0.5) = 0.5.
%		\end{equation*}
%		Note how $\text{Human}(a)$ is not considered for this computation, since it does not appear in $c$.
%		
%	\end{exampleblock}
%\end{frame}



\begin{frame}{KENN: Intuition}

Given the vector of predictions of the NN $y$, KENN computes the final vector of predictions as follows: $$y' = y +\sum_{c\in \mathcal{K}} w_c \cdot \delta^c$$
		where, for each $c \in \mathcal{C}$:
		\begin{itemize}
			\item $\delta^c$ improves the truth value of $c$, keeping $\|\delta^c\|_2$ minimal;
				\item $w_c \in \mathbb{R}$ is the \textbf{clause weight}, a learnable parameter that quantifies the importance of clause $c$.
		\end{itemize}

	
	
\end{frame}



%\begin{frame}{Boosting Preactivations}
%	In practice, inside KENN the delta vectors are applied to the \textbf{preactivations} from the NN:
%	\begin{equation*}
%	y'=\sigma(z + \delta^c).
%	\end{equation*}
%	\pause
%	Then, in order to increase the satisfaction of the entire knowledge, all the deltas are aggregated by being summed together. The final prediction will be:
%	\begin{equation*}
%	y^{\prime}=\sigma\left(z+\sum_{c \in \mathcal{K}} \delta^{c}\right).
%	\end{equation*}
%\end{frame}


%\begin{frame}{KENN for relational data}
%	\begin{itemize}
%		\item KENN can deal also with binary clauses, meaning clauses that contain binary predicates and express \textbf{relations} between objects;
%		\item This is done by considering unary and binary clauses separately, and computing the final prediction as follows:
%	\end{itemize}
%	 
%	\begin{equation*}
%	y^{\prime}=\sigma\left(z+\sum_{c \in \mathcal{K}_{U}} \delta^{c}+\sum_{c \in \mathcal{K}_{B}} \delta^{c}\right)
%	\end{equation*}
%\end{frame}
%
%\begin{frame}{Relational data representation}
%		\begin{figure}[h]
%		\centering
%		\includegraphics[width=0.8\linewidth]{images/kenn_relational_representation2.pdf}
%	\end{figure}
%\begin{equation*}
%\left\{\begin{array}{l}
%y_{u}^{\prime}=\sigma(U+\delta U) \\
%y_{b}^{\prime}=\sigma(B+\delta B)
%\end{array}\right.
%\end{equation*}
%
%\end{frame}
%
%\begin{frame}{Relational data representation}
%	\begin{figure}[h]
%		\centering
%		\includegraphics[width=0.6\linewidth]{images/kenn_relational_global_chart.pdf}
%	\end{figure}
%\end{frame}

\section{Experiments on Collective Classification}

\begin{frame}{Citeseer Experiments}
	\begin{itemize}
		\item We tested KENN on a Collective Classification task;
		\item The \textbf{Citeseer Dataset} was used: citation network with $4732$ citations (edges) between $3312$ papers (nodes);
		\item The task is to predict the topic of each paper (6 possible topics).
	\end{itemize}

\begin{figure}
	\includegraphics[width=\linewidth]{images/citeseer_setup_pres2.pdf}
\end{figure}
\end{frame}

\begin{frame}{Comparison with previous literature}
	\begin{itemize}
		\item We also provide a comparison with two other NeSy models:
		\begin{itemize}
			\item \textbf{Semantic Based Regularization}\footnote{Diligenti, Michelangelo, Marco Gori, and Claudio Sacca. "Semantic-based regularization for learning and inference." Artificial Intelligence 244 (2017): 143-165.};
			\item \textbf{Relational Neural Machines}\footnote{Marra, Giuseppe, et al. "Relational neural machines." arXiv preprint arXiv:2002.02193 (2020). \vspace{18pt}};
		\end{itemize}
	\pause
	
%		\vspace{10pt}
		\item The same base NN and the same base knowledge are used.
		\pause

		\item The main evaluation metric is the \textbf{relative improvement} over the base NN accuracy;
		\pause

		\item Same experiments are performed over different sizes of the training set.
	\end{itemize}
	
	
\end{frame}

%\begin{frame}{Learning Paradigms}
%	\begin{figure}
%		\includegraphics[width=0.58\linewidth]{images/ind_vs_trans.png}
%	\end{figure}
%\end{frame}


\begin{frame}{Results}
	\begin{figure}
		\centering
		\includegraphics[width=0.9\linewidth]{images/deltas_inductive.pdf}
%		\caption{Relative improvements and comparison with the other methods.}
	\end{figure}
\end{frame}

\begin{frame}{Results}
	\begin{figure}
		\includegraphics[width=0.6\linewidth]{images/histograms_inductive.pdf}
	\end{figure}
\end{frame}


\begin{frame}{Clause Weights Learning}
	\begin{figure}
		\includegraphics[width=0.41\linewidth]{images/scatter_25.pdf}%
		\hspace{5pt}
		\includegraphics[width=0.41\linewidth]{images/scatter_50.pdf}
		\includegraphics[width=0.41\linewidth]{images/scatter_75.pdf}%
		\hspace{5pt}
		\includegraphics[width=0.41\linewidth]{images/scatter_90.pdf}
	\end{figure}
\end{frame}


%\begin{frame}{Results Transductive Paradigm}
%	\begin{figure}
%		\centering
%		\includegraphics[width=\linewidth]{images/deltas_transductive.pdf}
%		\caption{Relative improvements for the Transductive Paradigm}
%	\end{figure}
%\end{frame}

\section{Explainability in KENN}

%\begin{frame}{Explainability}
%	\begin{definition}[Explainability]
%		We define explainability in the context of supervised ML
%		as the \textbf{generic process by which we extract any kind of explanation from a model}. This can
%		be done by exploiting the natural properties of the model (in which case, such a model can be
%		called explainable), or by devising techniques to extract explanations from any model.
%	\end{definition}
%
%\end{frame}

\begin{frame}{Explainability}
	In XAI, two main paradigms for explainability are distinguished: 
	\begin{itemize}
		\item \textbf{Transparency}
		\item \textbf{Post-hoc explainability}
	\end{itemize}
	\begin{figure}
		\includegraphics[width=\linewidth]{images/trasparency_vs_posthoc.png}
	\end{figure}
\end{frame}

%\begin{frame}{Activation Maximization}
%	\begin{figure}
%		\includegraphics[width=\linewidth]{images/activation_maximization.png}
%	\end{figure}
%\begin{equation*}
%x_{i}^{*}=\max _{x} \log p\left(\omega_{i} \mid x\right)-\lambda\|x\|^{2}
%\end{equation*}
%\end{frame}
%
%\begin{frame}{Local Explanations: saliency maps}
%	\begin{figure}
%		\includegraphics[width=0.8\linewidth]{images/explanation_map_nn.png}
%	\end{figure}
%	\begin{equation*}
%	R(x)_{i}=\left(\frac{\partial f}{\partial x_{i}}\right)^{2}
%	\end{equation*}
%\end{frame}

\begin{frame}{Explainability in KENN}
KENN can be considered a \textbf{partially transparent} model:
\begin{itemize}
	\item A KENN layer will always be based on the prediction of a base NN, which will always be an inherently opaque model;
	\item On the contrary, everything happening inside the KENN layer is transparent;
	\item The explanations will only regard the knowledge enforcement stage.
\end{itemize}
\end{frame}

%\begin{frame}{Local explanations from a single clause}
%	\begin{figure}
%		\includegraphics[width=\linewidth]{images/explainability_example0.pdf}
%	\end{figure}
%\end{frame}

\begin{frame}{Local explanations from a single clause}
	$$\neg \operatorname{Dog}(a) \vee \operatorname{Animal}(a) $$ \pause
	\begin{figure}
		\includegraphics[width=0.9\linewidth]{images/explainability_example1.pdf}
	\end{figure}
%\begin{equation*}
%\delta^c = 1 \cdot \operatorname{softmax}(0.2,0.3)
%\end{equation*}
\end{frame}

\begin{frame}{Local explanations from a single clause}
	$$\neg \operatorname{Dog}(a) \vee \operatorname{Animal}(a) $$
	\begin{figure}
		\includegraphics[width=0.9\linewidth]{images/explainability_example2.pdf}
	\end{figure}
\pause
Since the NN was confident that $a$ is not an Animal, the truth value for $a$ being a dog should decrease.
\end{frame}

\begin{frame}{Assessing impact of more clauses}
	\begin{itemize}
		\item In real use cases, we might have hundreds or thousands of clauses or samples $\Rightarrow$ one by one examination of each sample is not feasible; \pause
		\item We need ways to assess how the knowledge is modifying the base NN predictions, from a macroscopic point of view. Given any $\mathcal{C} \subseteq \mathcal{K}$ we might want to know: \pause
		\begin{itemize}
			\item if, and where those clauses provided a positive or negative contribution;\pause
			\item if and where there is any conflict between the formulas inside $\mathcal{C}$.
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Evaluation metrics}
\begin{block}{Improvement Score}
	Given $\mathcal{C} \subseteq \mathcal{K}$, the improvement score quantifies the positive (or negative) contribution of $\mathcal{C}$ for sample $x$ and is defined as follows:
	$$I S(x, \mathcal{C})=\sum_{i=1}^{m} \delta_{i} \cdot l_{i}.$$
\end{block}

\pause
\begin{figure}
	\includegraphics[width=0.95\linewidth]{images/ordering_metrics0.pdf}\pause
	\vspace{10pt} 
	\includegraphics[width=0.95\linewidth]{images/ordering_metrics1.pdf}
\end{figure}
	
\end{frame}

\begin{frame}{Evaluation metrics}
	\begin{block}{Disagreement Score}
		We first define the disagreement vector:
		$$D V(x, \mathcal{C})=\sum_{c \in \mathcal{C}}\left|\delta_{c}\right|-\left|\sum_{c \in \mathcal{C}} \delta_{c}\right|.$$
		
		Starting from $DV(x,C)$ we can finally define the disagreement score for a specific subset of predicates $\hat{\mathcal{P}} \subseteq \mathcal{P}$:
		$$D S(x, \mathcal{C}, \hat{\mathcal{P}})=\sum_{i\in \hat{\mathcal{P}}} D V(x, \mathcal{C})_{i}.$$
	\end{block}


\end{frame}

\begin{frame}{Conclusions}
	\begin{enumerate}
		\item Experimental results show that KENN outperforms other NeSy methods for the collective classification task;
		\item Further experiments show a correlation between the clause weights and the satisfaction of the clause in the training data;
		\item KENN is inherently a transparent NN layer: explanations can be easily extracted in a understandable and human readable form;
		\item We proposed two evaluation metrics which can be used for debugging purposes.
	\end{enumerate}
\end{frame}

\definecolor{unipd}{rgb}{.7,.105,.109}

\begin{frame}{}
	\centering \Large
	\textcolor{unipd}{\textbf{Thank you for your attention}}
\end{frame}

\appendix
\backupbegin

\begin{frame}{Appendix: Clause Enhancer}
	\begin{figure}
		\includegraphics[width=0.9\linewidth]{images/CE_presentation.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Appendix: Knowledge Enhancer}
	\begin{figure}
		\centering
		\includegraphics[width=0.55\linewidth]{images/KE.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Appendix: KENN for relational data}
	\begin{figure}
		\includegraphics[width=0.9\linewidth]{images/kenn_relational_representation2.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Appendix: KENN for relational data}
	\begin{figure}
		\includegraphics[width=0.65\linewidth]{images/kenn_relational_global_chart.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Appendix: preactivations vs activations}
	\begin{figure}
		\includegraphics[width=0.75\linewidth]{images/preac_deltas_example.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Example: truth value of a clause}
	\begin{figure}
		\includegraphics[width=\linewidth]{images/example_clause_boosting0.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Example: truth value of a clause}
	\begin{figure}
		\includegraphics[width=0.6\linewidth]{images/tvalues_literals.pdf}
	\end{figure}
\end{frame}
\begin{frame}{Increasing satisfaction of a single clause}
	\begin{figure}
		\includegraphics[width=0.499\linewidth]{images/boost_options2.pdf}
	\end{figure}
\pause
%	Given the vector of truth values of the literals $z_c$, this delta is coputed as follows:
	\begin{equation*}
	\delta_{s}^{w_{c}}\left(z_{c}\right)=w_{c} \cdot \operatorname{softmax}\left(z_{c}\right)
%	\label{eq:clause_enhancement}
	\end{equation*}
\end{frame}


\backupend








\end{document}
