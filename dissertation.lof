\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Intuitive representation of explanations coming from transparent models (left) vs. explanations coming from post-hoc interpretability techniques (right). Transparent models are inherently interpretable by their design; they can be inspected and explanations can be deduced from them. Post-hoc interpretability, on the other hand, refers to a wide range of techniques with which explanations are extracted by any already trained model. \relax }}{9}{figure.caption.13}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Process of image manipulation shown in \cite {mordvintsev2015inceptionism}. From an intuitive point of view, starting from existing real images, the network is asked to enhance the features of the image that resemble the desired object (in this case, starting from an image of a tree, start looking for features that resemble buildings). This process is then repeated, creating a positive feedback loop. As a result, the features of the desired objects appear seemingly out of nowhere.\relax }}{11}{figure.caption.14}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces This image illustrates the actual deltas produced by KENN, which are the $\delta ^f$, opposed to the actual delta produced on the activations, which is $\delta ^g$. As we said, $\delta ^g$ is not produced directly by the model but it is indirectly \textit {induced} by the application of $\delta ^f$ on the preactivations. This also illustrates how, thanks to the shape of the sigmoid activation function, the same delta on the preactivation produces a different delta at the activations level: the closer the preactivations to zero, the highest the modification on the final predictions. \relax }}{27}{figure.caption.15}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Summary of all the steps needed to produce $\delta ^c$, the vector of deltas derived from a single clause. We refer to this process as \textit {clause enhancement}.\relax }}{29}{figure.caption.16}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Detailed depiction of the Clause Enhancer for the clause.... todo\relax }}{31}{figure.caption.17}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces The KE architecture... todo\relax }}{32}{figure.caption.18}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Representation of relational data inside KENN\relax }}{32}{figure.caption.19}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces Deltas for the inductive learning task. $95\%$ confidence intervals.\relax }}{38}{figure.caption.22}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces Deltas for the transductive learning task. $95\%$ confidence intervals.\relax }}{38}{figure.caption.23}%
\contentsline {figure}{\numberline {3.8}{\ignorespaces Histograms showing the distribution of the accuracies for all the different $500$ runs, for the inductive case. On the left, the accuracies of the base NN vs accuracies of KENN. On the right the distribution of the difference between the NN accuracy vs KENN accuracy.\relax }}{39}{figure.caption.24}%
\contentsline {figure}{\numberline {3.9}{\ignorespaces Histograms showing the distribution of the accuracies for all the different $500$ runs, for the transductive case. On the left, the accuracies of the base NN vs accuracies of KENN. On the right the distribution of the difference between the NN accuracy vs KENN accuracy.\relax }}{40}{figure.caption.25}%
\addvspace {10\p@ }
